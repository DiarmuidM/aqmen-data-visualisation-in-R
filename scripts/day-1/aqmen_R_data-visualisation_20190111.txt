# 
# AQMEN (Data Science for Social Research)
# http://www.aqmen.ac.uk/
#
# 
# Data Visualisation
# 
# R Workshop (March 2019)
# 
# A three day hands-on workshop led by Dr Diarmuid McDonnell and Professor Vernon Gayle, University of Edinburgh.
# 
# 
# Topics: 
# 
# Visualising data is emerging as a key component in effectively communicating research results and evidence. 
# This three-day workshop will provide a comprehensive introduction for individuals wishing to learn 
# how to design, produce and interpret data visualisations. 
#
#
# Rationale: 
# 
# The Industrial Strategy recognises that a major challenge facing UK businesses and industry is how best to utilise big data 
# to improve economic performance and increase productivity. A substantial barrier to exploiting the potential offered by 
# emerging forms of big data is the lack of a suitably trained workforce with appropriate analytical skills. 
#
# Many statistical analysis techniques used in the social sciences are also suitable for working with big data in non-academic settings, 
# however social science graduates often lack experience in applying their skills and knowledge in non-academic research domains.
#
#
# Advice:
#
# The workshop is intended for people who have little prior experience of R.
#
# The aim of the workshop is to equip you with a proficiency in data visualisation using R as rapidly and painlessly as possible.
#
# Therefore, be good to yourself: we explore a multitude of useful data visualisation techniques that often take most of a semester to cover. 
#
# It will NOT be possible to learn everthing in three days (drinks on us if you prove us wrong).
#
# Please be patient. Computers often go wrong.
#
# Please asks the instructors for help.
#
# Feel free to work in pairs during the pratical sessions.
#
# Not all of your questions will be answered but we will help as much as we can.
#
# Good luck.
#
##############################################


##############################################

# Outline of Activities #

# The workshop is based around a series of activities that involve the use of R for organising and enabling administrative data for statistical analysis:

#	1. Getting Started with R: a quick introduction to the R programming language and various data types [ACT001]

#	2. Wrangling Data for Graphing: how to organise and enable data for visualisation [ACT002]

#	3. The Grammar of Graphics: how to create graphs in a logical and layered manner using ggplot2() [ACT003]

#	4. Graphs in Action: how to construct a multitude of plot types in R [ACT004]

#	5. Refining Graphical Presentations: how to add and tweak the design elements of your plot [ACT005]

#	6. Communicating Analytical Results: how to plot the results of statistical models [ACT006]

#	7. Further Adventures in Graphing: an overview of new plot types, themes and approaches [ACT007]

#	8. Hackathon: two blocks of time where participants will tackle a data visualisation challenge using administrative data [ACT008]

# If you want to jump to a section: press Ctrl + F and search for the activity code e.g. [ACT001].


##############################################


##############################################
#
#
# We suggest that you make a copy of this file.
#
#
# ##############################################
# # IT IS IMPORTANT THAT YOU READ THIS HANDOUT #
# # AND FOLLOW THE R FILE LINE BY LINE! #
# ##############################################
# 
# The file is sequential. It MUST be run line by line. 
# Many of the commands will NOT run if earlier lines of commands have not been executed.
#
# Anotate your new copy of the file as you work through it with your own notes 
# (use "#" to comment out your notes).
#
#
# Throughout the file there are markers requiring your input:
#	- TASK: a coding task for you to complete
#	- QUESTION: a question regarding your interpretation of some code or a plot
#	- EXERCISE: a data visualisation challenge for you to complete at the end of an activity
#
# 
# ON WITH THE SHOW!
#
#
##############################################


##############################################

# 0. Software Demonstration #

# 0.1 System Setup #

# Create a R project folder/directory

# Open RStudio and follow these instructions:
# 	- File > New Project
#	- Create project directory

getwd() # tells us the current working directory i.e. workspace
# setwd("C:/Users/mcdonndz-local/Desktop/temp") # set the working directory to a specified directory; however we have no need to
# do this as we have already set up a directory to store all of the components of our R project.

folders = c('data_raw', 'data_clean', 'figures', 'temp', 'logs') # create a list with folder names
for(f in folders) {
  print(f)
  dir.create(f)
} # take a look at the bottom right-hand panel in RStudio (or the directory on your machine) to check if the folders were created

# Creating and saving files:
data <- file.create("./temp/sampdata.csv")
write.csv(data, "./temp/sampdata_20190321.csv")
# Note the use of "." at the beginning of the file path; this signifies that the current working directory
# should form the first part of the path without needing to be explicitly stated. This is an example of
# using relative file paths and is considered good practice.

# List all files in our working directory:
dir() # list all files in a directory
head(dir(recursive = TRUE)) # list all files in a directory (including its subdirectories); head() restricts the output to the first few results
dir(pattern = "\\.csv$", recursive = TRUE) # find all files that end in ".csv"
# The above command used regular expressions to detect patterns in text.

file.info("./data_raw/sampdata.csv") # displays some basic file information 
# (e.g. size, whether it is a folder, created and modified times)

# That's enough file management for now. There are lots of other tasks we can perform, such as copying, moving, deleting,
# opening, checking if a file exists etc, that we do not cover here: see [http://theautomatic.net/2018/07/11/manipulate-files-r/]

# TASK: move the files from the workshop Dropbox folder to the "data_raw" directory you just created.

# R functions used in this section:

#	- getwd() - provide working directory location
#	- c() - create a vector of values or objects
#	- print() - display information on the console
#	- file.create() - create a file
#	- write.csv() - export a csv file to a directory
#	- dir.create() - create a directory
#	- dir() - list all files in a directory
#	- head() - list the first few rows (values) of a data frame (object)
#	- file.info() - display information about a file


# 0.2 Installing Packages #

# The real power of using R for data wrangling and analysis comes from the universe of user-written packages that are available.
# A package bundles together code, data, documentation, and tests and provides an easy method to share with others.

# Packages represent both a blessing and a curse: a blessing because it is unlikely you won't be able to find a function you need for your analysis;
# a curse because it adds a bit of administrative burden to your workflow (i.e. find a package, install it, load it, use it). Also,
# help documentation is wildly inconsistent across packages. 

# A package only needs to be installed once, but you will need to load it in every time you launch an R session.

my_packages <- c("tidyverse", "car", "haven") # create a list of desired packages

install.packages(my_packages, repos = "http://cran.rstudio.com") # install packages from the CRAN repository

installed.packages() # check which packages have been installed

.libPaths() # check which folder the packages are downloaded to


# 0.3 Loading Packages #

library(tidyverse) # load in the "tidyverse" package of data wrangling functions
?tidyverse
vignette("tidyverse")

# A final note about packages: you'll see mention of performing functions or tasks using base R. This means drawing on the functions that come
# as standard with your version of R. install.packages() and write.csv() are examples of base R functions.

# For the purposes of data wrangling (and most other data analysis tasks, frankly), we will not use base R functions; the reasons will become clear
# as we progress but it is worth noting that there is more than one way to skin a cat.

# R functions used in this section:

#	- install.packages() - install R packages
#	- installed.packages() - display installed R packages
#	- .libPaths() - display directory of installed R packages
#	- library() - load in R package

##############################################


##############################################


# 1. Getting Started with R #

# R is a programming language. It has rules, packages, syntax, complexities, idiosyncracies...
# It is not particularly easy to learn, nevermind master. It can seem as if you have to learn everything in order
# to do anything!

# Persevere: like any language, once you grasp the building blocks you will begin to feel comfortable. All of the fancy models, code and graphs
# that make it into journal articles, textbooks, presentations etc are just extensions and flourishes added on top of the basic functions and rules.

# The important thing is to expect failure and react accordingly (just like an astronaut).


# 1.1 Comments #

# This is a comment
## This is also a comment
###### ...you get the idea

# Comments are an important means of documenting your workflow and ensuring others (including future-you) can reproduce your work.

# In R studio, you can create multiline comments by highlighting the text and pressing Ctr + Shift + C. For example:

This should be a comment and not code.
Excuse me, did you hear me?
HELLLOOOO!
How rude...


# 1.2 Writing Code #

print("Hello World!") # display a message to the console

# To execute (i.e. run) the above code, highlight it and press Ctrl + Enter, or the Run icon in the top-left panel in RStudio.

# TASK: print your own personalised message to the console.


# 1.3 Data Types #

# Variables are known as 'objects' in R and can store a wide variety of data types:
# - numeric
# - string
# - boolean etc

# Each data type can have different classes i.e. numeric has integer and double (e.g. decimal).

# We assign a value to an object using the "<-" operator. We can also use "=" but this is best avoided as the equals sign has another use
# and "<-" is considered standard practice in R.

# 1.3.1 Numeric #

x <- 5 # Integer
y <- 5.5 # Double or Float

# Notice how RStudio doesn't print the value of x or y. To evaluate the assigment you need to call the object:
x
y

# Assign and evaluate in a single command:
(x <- 5)
# Our advice is to keep assignment and evaluation commands separate (those parentheses can add confusion and lead to errors) but the choice is yours...

print(x + y) # print ensures the result is displayed in the console or output window

# We can compare objects using a set of comparison operators:
x == y
x < y
x > y
x != y
x >= y
x <= y
# TASK: document what each of the comparison operators does.

a <- c(1, 4, 9, 12)
b <- c(4, 4, 9, 13)
a == b # compares each number in the vector to its corresponding number in the other vector

# Note that logical values TRUE and FALSE equate to 1 and 0 respectively, allowing us to perform arithmetic operations using these results:
sum(a == b) # 2 instances where the elements of the vector are equal

# To test if two objects are exactly equal:
identical(x, y)

print(typeof(x))
print(typeof(y)) # R stores numbers as a double by default; we need to be specific when assigning the object's value(s):

rm("x") # remove the objects from R
rm("y") # check the environment pane in the top-right hand corner of RStudio to see what objects remain in the global environment

x <- 5L # rather counterintuitively given that it's a letter, the "L" suffix ensures a number is stored as an integer
y <- 5.5
print(typeof(x)) # Now it is stored explicitly as an integer; in practice you often do not need to worry about this
print(typeof(y)) 

# Another approach is to convert an existing object:
int_var <- 20
int_var <- as.integer(int_var)
print(typeof(int_var))

# Vectors

vec <- 1:10
print(vec) # creates a vector from 1 to 10; a vector is a list of values stored in a single object
vec[1] # return the first element in vec
vec[1:5] # return the first five elements in vec
vec[-2] # return the values of the vector, excluding the second element
vec[-1:-5]
# TASK: describe the results of "vec[-1:-5]".

# The above commands are known as 'slicing' i.e. accessing a particular element(s) in a vector.

# You can also store objects as a vector:
ovec <- c(x, y) # combine the objects "x" and "y" in a vector called "ovec"
ovec

# You can count the number of elements in a vector:
length(vec)

# You can also drop elements:
vec <- vec[-2] # note that we overwrite the existing object; we could just as easily assign a new object to preserve the original
vec2 <- vec[-(4:6)] # drop 4-6 from the vector

# We can perform calculations with vectors:
a <- c(1, 2, 3, 4, 5)
b <- c(6, 7, 8, 9, 10)
c <- c(1, 2, 3)

a + b # adds each element of the vectors together in order (i.e. 1 + 6, 2 + 7 etc)
# This is known as vectorization and is a very useful property of R.

a + c # generates a warning that the vectors are not multiples of each other (i.e. one has 5 elements, the other 4)
# When vectors are of unequal length, the shorter vector is "recycled" i.e. goes back to the start.

# Generate a sequence of numbers

sequence <- seq(from = 1, to = 100, by = 5)
print(sequence) 
# TASK: describe what the seq() function is doing above.
# TASK: create a sequence of numbers that starts at 55, ends at 7000, and increases by 55 each time.

# Create a sequence based on repeating or replicating the numbers
repetition <- rep(1:10, each = 10)
print(repetition)

# Generating sequences of random numbers

# This is a useful function for performing simulations or generating data for testing ideas and techniques

# Generate 100 random numbers between 0 and 25 from a uniform distribution i.e. each number has an equal probability of being selected
runif(100, min = 0, max = 25)

# Generate 100 random numbers between 0 and 25 (with replacement)
sample(0:25, 100, replace = TRUE)

# Generate 100 random numbers between 0 and 25 (without replacement)
sample(0:25, 100, replace = FALSE)

# QUESTION: why can we not sample 100 numbers from this range without replacement?

# Generate 1000 random numbers from a normal distribution with given mean and standard deviation
normdist <- rnorm(1000, mean = 0, sd = 1)
hist(normdist) # approximately normal 
print(summary(normdist))

# Generate CDF probabilities for value(s) in vector q
pnorm(0.5, mean = 0, sd = 1)

# Generate quantile for probabilities in vector p
qnorm(0.5, mean = 0, sd = 1)

# Generate density function probabilites for value(s) in vector x
dnorm(0.5, mean = 0, sd = 1)

# Generate a vector of length n displaying the number of successes from a trial of size = 100 with a probabilty of success = 0.5
rbinom(10, size = 100, prob = 0.5)
# QUESTION: how many successes were there in 10 trials, each with a sample size of 100?

# Generate a vector of length n displaying the random number of events occuring when lambda (mean count) equals 4.
rpois(20, lambda = 4)
# TASK: vary the number of expected events and interpret the results.

# We can reproduce random numbers by setting the seed:
set.seed(1) # name the random sample "1"
rsamp1 <- rnorm(n = 10, mean = 0, sd = 1)
set.seed(1)
rsamp2 <- rnorm(n = 10, mean = 0, sd = 1)
print(rsamp1)
print(rsamp2) # produces the same values in each random sample

# Rounding numbers

x <- c(1, 1.35, 1.7, 2.05, 2.4, 2.75, 3.1, 3.45, 3.8, 4.15, 4.5, 4.85, 5.2, 5.55, 5.9)
print(x)

# Round to the nearest integer
round(x) # note how the original object is not altered - run the command "print(x)" to check

# Round up
ceiling(x)

# Round down
floor(x)

# Round to a specified decimal
round(x, digits = 1)


# 1.3.2 Strings #

# Strings (text) are stored in the character class in R.

a <- "learning to create" # create string a
b <- "character strings" # create string b
paste(a, b) # combine the strings

# Paste character and number strings (converts numbers to character class)
paste("The life of", pi)

# Paste multiple strings
paste("I", "love", "R")

# Paste multiple strings with a separating character
paste("I", "love", "R", sep = "-")

# Converting to strings

a <- "The life of"
b <- pi
is.character(b) # check if b is a string
c <- as.character(b)
is.character(c)

# Printing strings

print(a)
print(a, quote = FALSE) # easier to use the command "noquote(a)"
noquote(a)

cat(a)
cat(a, "Riley") # the cat function is useful for printing multiple objects in a readable format
cat(letters)

x <- "Today I am learning how to print strings."
y <- "Tomorrow I plan to learn about something else."
z <- "The day after that I will take a break and drink a beer."
cat(x, y, z, fill = 1) # the fill option specifies line width

# Substituting strings and numbers

x <- "The R package is great"
sprintf("You know what? %s", x) # think of "%s" as a placeholder for a string stored in an object
TASK: call the help documentation for the "sprintf()" function.

y <- 0
sprintf("You know what? I had %d beers last night", y)
sprintf("Here are some digits from Pi: %f", pi) # "%f" is a placeholder for a number stored in an object

# Counting string elements and characters

length("How many elements are in this string?")
length(c("How", "many", "elements", "are", "in", "this", "string?"))

nchar("How many characters are in this string?")
nchar(c("How", "many", "characters", "are", "in", "this", "string?"))
# Counting elements and characters becomes very useful when constructing loops.

# Special characters

string2 <- 'If I want to include a "quote" inside a string, I use single quotes'
string3 <- "\""
string4 <- "\'" # if we want to include a single or double quote in our string we use the backslash (\) to escape the character
TASK: include a backslash in a string.

x <- c("\"", "\\")
x
writeLines(x) # beware that the printed representation of a string is different from the contents of the string itself
# Special characters are very useful in R but they can throw a spanner in the works; we'll deal more with them later in the workshop.

# String manipulation with stringr

# We can perform a lot of the core string manipulation tasks (e.g. removing whitespace, converting to lowercase etc)
# using base R functions. However we will use a package that simplifies the syntax: stringr

help("stringr")

# All functions in stringr start with str_ and take a vector of strings as the first argument:

x <- "Hello, this is a run-of-the-mill string."
str_length(x)
str_c(x, " ", "Not very interesting at all.") # combine strings
str_sub(x, 1, 10) 
# QUESTION: what is the str_sub function doing to the string?

y <- c("Hello", "This", "is a bog standard", "string")
str_subset(y, "[aeiou]") # returns strings matching the pattern i.e. contain a vowel
str_subset(y, "[qrstuvwxyz]")

# Change text to upper, lower or title case
uc <- "DOWN WITH THAT SORT OF THING"
lc <- "careful now"
str_to_upper(lc)
str_to_lower(uc)
str_to_title(uc) 
# QUESTION: what tv show are those strings referencing an iconic moment from?

# String matching

str_detect(y, "[aeiou]") # tells you if there’s any match to the pattern
str_count(y, "[aeiou]") # counts how many vowels are in each string
str_locate(y, "[aeiou]") # gives the position of the first match
str_extract(y, "[aeiou]") # extracts the text of the first match
str_match(y, "(.)[aeiou](.)") # extract the characters on either side of the vowel
str_replace(y, "[aeiou]", "?") # replace first match with a specified character
str_split(x, "") # split a string into individual characters based on a specified separator
str_dup(x, times = 10) # duplicates the string n times

# Removing leading and trailing whitespace

text <- c("Text ", " with", " whitespace ", " on", "both ", " sides ")
print(text)

# Remove whitespaces on both sides
str_trim(text, side = "both") # other options include "right" and "left"
str_pad("beer", width = 10, side = "left") # add whitespace on the left of the string

# Set operations for strings

set_1 <- c("lagunitas", "bells", "dogfish", "summit", "odell")
set_2 <- c("sierra", "bells", "harpoon", "lagunitas", "founders")
union(set_1, set_2) # list all individual elements from the sets
intersect(set_1, set_2) # list all common elements from the sets
setdiff(set_1, set_2) # returns elements in set_1 not in set_2; swap order of sets

# To test if two vectors contain the same elements regardless of order use setequal():

set_3 <- c("woody", "buzz", "rex")
set_4 <- c("woody", "andy", "buzz")
set_5 <- c("andy", "buzz", "woody")
setequal(set_3, set_4)

set_6 <- c("woody", "andy", "buzz")
identical(set_4, set_6) # check if sets are exactly equal (elements and order)

# Identifying if element is in string
good <- "andy"
bad <- "sid"
is.element(good, set_5) # is the word "andy" in set_5?
good %in% set_5 # same as above
# TASK: see if the word "sid" is in set_3.

# Sort a string
sort(set_5)
sort(set_5, decreasing = TRUE)

# That's enough of strings. The information they contain can be of considerable interest for quantitative data analyses and it is worth
# beginning familiar with their storage and manipulation in advance of more sophisticated analyses (e.g. Natural Language Processing).


# 1.3.3 Categorical Variables #

# Known as factor variables in R (and other packages like Stata etc), 
# they group observations into exhaustive and mutually exclusive categories.
# We'll use the forcats package to work with categorical variables in R:
library(forcats) # not part of the core tidyverse package so we need to load it in separately

# Create a factor

x <- c("male", "female", "female", "male", "female") # list of observations for biological sex
sex <- factor(x)
sex
# QUESTION: how many observations and levels are there for this factor variable?

class(sex) # confirm it is a factor variable
unclass(sex) # show the underlying values of this variable: female = 1, male = 2 (numeric values are attached to categories alphabetically by default);

# We can change the order in which numbers are attached to categories by specifying the "levels" option:
sex2 <- factor(x, levels = c("male", "female"))
levels(sex2)  # display the levels (i.e. categories) of this variable
summary(sex2) # summarise the variable i.e. frequency count

# We can convert an existing list of strings to a factor variable:
group <- c("Group1", "Group2", "Group2", "Group1", "Group1")
group2 <- factor(group)
levels(group2)

# Instead of numbering categories alphabetically, we can do so according to when a factor first appears:
las <- c("Glasgow", "Edinburgh", "Aberdeen", "Glasgow", "Orkney", "Edinburgh")
cat_las <- factor(las, unique(las))
attributes(cat_las)
unclass(cat_las)

# Ordering factor variables
ses <- c("low", "middle", "low", "low", "low", "low", "middle", "low", "middle",
         "middle", "middle", "middle", "middle", "high", "high", "low", "middle",
         "middle", "low", "high")
ses <- factor(ses, levels = c("low", "middle", "high"), ordered = TRUE)
print(ses) # categories are ordered from "low" to "high"
factor(ses, levels = rev(levels(ses))) # you can also reverse the order of levels if desired

# Recoding categorical variables

# The "plyr" package is useful for this task:
new_ses <- plyr::revalue(ses, c("low" = "small", "middle" = "medium", "high" = "large"))
print(new_ses)
levels(new_ses)
# Note that using the :: notation allows you to access the revalue() function without having to fully load in the plyr package.
# There are other ways of recoding categorical variables, none of which (in my opinion) are as easy as Stata's approach.

# Dropping categories with no observations

ses_2 <- ses[ses != "middle"] # create a new variable where ses does not equal the value "middle"
summary(ses_2)
droplevels(ses_2)


# 1.4 Saving Files #

# I need to figure this out in the context of creating R project files.


# 1.5 Getting Help #

help.start() # provides general help links
help.search("regression") # searches the help system for documentation matching a given character string

help("strtrim") # finds help documentation on the "strtrim" function
?strtrim # another way of searching for help
example("strtrim") # display example code using this function
# In RStudio, you can also highlight the function in your code and press F1.

help(mean) # let's dissect the help material for the mean() function
vignette("dplyr") # some of the better-documented packages provide detailed examples of how to use its functions

# Can't quite remember the name of an object or function?
apropos("sum") # returns all objects in the global environment that contain the text "sum"

# If you need help from the web then [insert chosen search engine] is your friend.
# Likewise, the Stackoverflow website is an excellent source of help on many programming languages and data wrangling/analysis problems.
# If you experience an error message then follow the above advice by searching for the exact message you receive i.e. don't paraphrase
# your issue.


# 1.6 Keyboard Shortcuts #

# To execute R code: highlight the syntax and press Ctrl + Enter.

# To execute the entire R script (i.e. all of the code in one): press Ctrl + Shift + S.

# To insert the assignment operator (i.e '<-'): press Alt + minus key (-).

# To autocomplete your syntax: start typing the name of an object/function and press TAB.

# To insert the pipe operator (%>%): press Ctrl + Shift + M.


# 1.7 Troubleshooting #

# If you run some code and nothing happens, check the console (bottom-left pane in RStudio): if you see a plus sign (+) then
# R thinks you haven't finished writing the command and expects more code. If this happens then press the ESC key to cancel the
# command.

# R is case sensitive e.g. 'View(data)' displays the data set in a window (similar to the 'browse' function in Stata);
# 'view(data)' does nothing.


# 1.8 Debugging #

# This is the computer science term for dealing with code issues. R likes to tell you when something is not quite right,
and not always in an intelligble manner. As progress with this workshop, you are likely to encounter the following results:
#	- message: R is communicating a diagnostic message relating to your code; your commands still execute.
#	- warning: R is highlighting an error/issue with your code, your commands still execute but the warnings need addressing.
#	- error: R is telling you there has been a fatal error with your code; your commands do not execute.

# Let's look at a simple example:
log(-1) # take the natural log of -1
# This warning tells you that the output is missing i.e. there is no natural log of a negative number.
warning() # displays the warnings associated with the most recently executed block of code

# You'll encounter plenty of messages, warnings and errors over the course of this workshop. For now, here is some general
# advice from Peng (2015) regarding what questions to ask when debugging:
#	- What was your input? How did you call the function?
#	- What were you expecting? Output, messages, other results?
#	- What did you get?
#	- How does what you get differ from what you were expecting?
#	- Were your expectations correct in the first place?
#	- Can you reproduce the problem (exactly)?


# 1.9 Environment Objects #

# We can remove some of the objects we've created (i.e. delete variables):
ls() # list existing objects

a <- "I am a useless object"
rm("a") # delete the object "a"

exists("x") # check if the object "x" exists
rm(c("x", "y")) # you can remove multiple objects by using the "c()" function

history(Inf) # displays all of the commands executed in this R session


# 1.10 Workspace Options #

help(options)
options() # wide range of options for displaying results etc
options(digits=3) # change a specific option (i.e. number of digits to print on output)
options(max.print = 9999) # set maxmimum number of rows to print to the console as 9999


# 1.11 Data Visualisation Examples #

# Congratulations on getting through the technical (boring) bit of the activity. To whet your appetite, here are
# some examples of the techniques you will learn over the course of the workshop.

# Import data #

library(readr)

auto <- read_csv("./data_raw/auto.csv") # the (in)famous auto data set from Stata
str(auto)
auto

# Some simple graphs #

# DMD TASK: ADD PLOTS FOR EACH COMBINATION OF VARIABLE TYPES E.G. CONTINUOUS BY CATEGORICAL (BOXPLOT), CAT BY CAT ETC

# A simple box graph

str(auto$price)
sum(is.na(auto$price)) # no missing values

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_boxplot(mapping = aes(x = "", y = auto$price)) # single boxplot (no x axis)
ggplot(data = auto) + geom_boxplot(mapping = aes(x = auto$foreign, y = auto$price)) # two boxplots


# A simple pie chart

# Pie charts are tricky to produce in R and with ggplot2, and thus are not recommended
# when a bar chart would suffice.


# A simple bar chart

class(auto$foreign) # stored as character data type - we want this to be a factor (categorical) variable
auto$foreign <- as.factor(auto$foreign) # coerce the foreign variable to be factor data type
levels(auto$foreign)
factor(auto$foreign) # that's more like it
unclass(auto$foreign) # Domestic = 1; Foreign = 2

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_bar(mapping = aes(auto$foreign)) 

ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$foreign)) # add colour to the bars based on categories of foreign
ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$rep78)) # add colour to the bars based on categories of rep78; the result is a stacked bar chart
ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$rep78), position = "fill") # stacked bars of same height
ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$rep78), position = "dodge") # place bars side-by-side


# A simple histogram

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg)) # narrows bins result in gaps
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg), binwidth = 4)
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg), binwidth = 4, fill = "red") # add colour to the graph

# Histogram diassgregated by a second variable:
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg, fill = auto$foreign), binwidth = 4)
# Note how we moved the fill option inside the aes() function.

# Density plot
ggplot(data = auto) + geom_density(mapping = aes(auto$mpg, fill = auto$foreign))


# A simple dot chart

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_dotplot(mapping = aes(x = auto$price))


# A simple count plot

ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color)) # a count plot is a good way of visualing the relationship between two categorical variables
# QUESTION: what are the most common combinations of diamond colour and quality?


# A simple scatterplot

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg))
# QUESTION: what is the association between miles per gallon and the price of a car?

ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg, color = foreign))
# QUESTION: what is the effect of specifying the color option in the aes() function?

ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg, size = foreign))
# QUESTION: is a categorical variable a good choice for mapping to the size aesthetic?

ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg, alpha = foreign))

ggplot(data = auto) + geom_point(mapping = aes(x =auto$price, y = mpg, shape = foreign))
ggplot(data = auto) + geom_point(mapping = aes(x =auto$price, y = mpg, shape = foreign, color = foreign))
# Note how we can combine different shapes and colours in a single plot.

# We've introduced a number of different aesthetic options in the above graphs:
#	- color
#	- shape
# 	- size
#	- alpha

# Keep these in mind as we progress, we'll also introduce other options that alter the display of the plots.















# Data Visualisation in R #
# Derived from Fogarty (2018) and Healy (2019)

# Data Visualisation is a key skill in social science data analysis, playing a vital role in theory development, 
# measurement, data exploration, and interpretation and communication of results. 
# It is an equally powerful and convincing approach to data analysis in industry.

# The aim of a visualisation is to present a great deal of often complicated/complex information in as clear, concise and
# intelligible way as possible; by and large, the consumer of a graph should not need to know the underlying logic and details
# of the analysis in order to understand the message conveyed by the visualisation.

#############

# ADD IN PRELIMINARIES FROM AQMEN DO FILES

##############


#### TASKS (DMD) #####

# Number the sections, starting from 0.
# Set up course directories for files.
# Match sections to course programme.
# Learn how to save R scripts.
# Add some material from data wrangling and predictive analytics workshops e.g. summary stats
# Add some material on RMarkdown for the final day!!

###########################

# Install and load in necessary packages

# A package only needs to be installed once, but you will need to load it in every time you launch an R session.
my_packages <- c("tidyverse", "broom", "coefplot", "cowplot",
                 "gapminder", "GGally", "ggrepel", "ggridges", "gridExtra",
                 "here", "interplot", "margins", "maps", "mapproj",
                 "mapdata", "MASS", "quantreg", "rlang", "scales",
                 "survey", "srvyr", "viridis", "viridisLite", "devtools") # create a list of desired packages

install.packages(my_packages, repos = "http://cran.rstudio.com") # install packages from the CRAN repository

devtools::install_github("kjhealy/socviz") # install socviz package from Github (an alternative to CRAN for storing packages)

library(ggplot2)
?ggplot2
# ggplot2 works by combining "layers" to create a visualisation i.e. data + variables/axes + graph/plot type + labelling etc
# It is an implementation of an approach known as the "grammar of graphics" (Wilkinson, 2005):

# "The grammar is a set of rules for producing graphics from data, taking pieces of data 
# and mapping them to geometric objects (like points and lines) that have aesthetic 
# attributes (like position, color and size), together with further rules for transforming 
# the data if needed (e.g. to a smoothed line), adjusting scales (e.g. to a log scale)
# and projecting the results onto a different coordinate system (usually cartesian)."
# (Healy, 2019)

# Note that we could use R's default plotting options (e.g. boxplot() etc) but we feel it is
# best to get into the habit of using the more flexible, varied and powerful options provided
# by ggplot2.

# Further reasons to like ggplot? The default display options are well-chosen and there are a multitude of options for refining plots;
# these features incorporate general rules for good visualisation as well as cognitive preferences for information presentation 
# i.e. accounts for how we perceive colour, shapes, text, length etc.

# 1. Import data #

library(readr)

auto <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.csv") # the famous sysuse data set from Stata
str(auto)
auto

# 2. Some simple graphs #

# DMD TASK: ADD PLOTS FOR EACH COMBINATION OF VARIABLE TYPES E.G. CONTINUOUS BY CATEGORICAL (BOXPLOT), CAT BY CAT ETC

# A simple box graph

str(auto$price)
sum(is.na(auto$price)) # no missing values

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_boxplot(mapping = aes(x = "", y = auto$price)) # single boxplot (no x axis)
ggplot(data = auto) + geom_boxplot(mapping = aes(x = auto$foreign, y = auto$price)) # two boxplots


# A simple pie chart

# Pie charts are tricky to produce in R and with ggplot2, and thus are not recommended
# when a bar chart would suffice.


# A simple bar chart

class(auto$foreign) # stored as character data type - we want this to be a factor (categorical) variable
auto$foreign <- as.factor(auto$foreign) # coerce the foreign variable to be factor data type
levels(auto$foreign)
factor(auto$foreign) # that's more like it
unclass(auto$foreign) # Domestic = 1; Foreign = 2

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_bar(mapping = aes(auto$foreign)) 

ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$foreign)) # add colour to the bars based on categories of foreign
ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$rep78)) # add colour to the bars based on categories of rep78; the result is a stacked bar chart
ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$rep78), position = "fill") # stacked bars of same height
ggplot(data = auto) + geom_bar(mapping = aes(x = auto$foreign, fill = auto$rep78), position = "dodge") # place bars side-by-side


# A simple histogram

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg)) # narrows bins result in gaps
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg), binwidth = 4)
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg), binwidth = 4, fill = "red") # add colour to the graph

# Histogram diassgregated by a second variable:
ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg, fill = auto$foreign), binwidth = 4)
# Note how we moved the fill option inside the aes() function.

# Density plot
ggplot(data = auto) + geom_density(mapping = aes(auto$mpg, fill = auto$foreign))


# A simple dot chart

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_dotplot(mapping = aes(x = auto$price))


# A simple count plot

ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color)) # a count plot is a good way of visualing the relationship between two categorical variables
# QUESTION: what are the most common combinations of diamond colour and quality?


# A simple scatterplot

x11() # opens a bigger window in which to display subsequent graphs
ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg))
# QUESTION: what is the association between miles per gallon and the price of a car?

ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg, color = foreign))
# QUESTION: what is the effect of specifying the color option in the aes() function?

ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg, size = foreign))
# QUESTION: is a categorical variable a good choice for mapping to the size aesthetic?

ggplot(data = auto) + geom_point(mapping = aes(x = price, y = mpg, alpha = foreign))

ggplot(data = auto) + geom_point(mapping = aes(x =auto$price, y = mpg, shape = foreign))
ggplot(data = auto) + geom_point(mapping = aes(x =auto$price, y = mpg, shape = foreign, color = foreign))
# Note how we can combine different shapes and colours in a single plot.

# We've introduced a number of different aesthetic options in the above graphs:
#	- color
#	- shape
# 	- size
#	- alpha

# Keep these in mind as we progress, we'll also introduce other options that alter the display of the plots.


# 3. ggplot() basics #

# data > mapping > geom > co-ordinates and scales > labels
# The above represents the basic syntax of a ggplot graph; let's look at an example to understand what is happening at each
# step and the final result:

library(gapminder) # load in gapminder data - see https://www.gapminder.org/
gapminder

# Create the base object for producing our graph (i.e. the data and mapping components):

p <- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap,
                          y = lifeExp))
# We pass some data to the ggplot() function, followed by mapping the variables in the data to the aesthetic of the plot
# i.e. how it is going to look.

# Next we specify what type of plot we would like to draw and add it to the base object we created ("p"):

p + geom_point() # geom_point() creates a scatterplot
# QUESTION: how would you describe the association between GDP and life expectancy based on this graph?

# Healy's (2019) summary of the graphing process in R:
# 	1. Tell the ggplot() function what our data is.
#	2. Tell ggplot() what relationships we want to see. For convenience we will put the results of the first two steps in an object called p.
# 	3. Tell ggplot how we want to see the relationships in our data (i.e. choose a geom).
#	4. Layer on geoms as needed, by adding them to the p object one at a time.
#	5. Use some additional functions to adjust scales, labels, tick marks, titles.

# Let's add an additional geom to the graph:

p + geom_point() + 
  geom_smooth() # 'smooth' the trend line 

# A quick (and important) aside: note how the '+' symbol appears at the end of the line. This is crucial as placing on the next line will result in the
# command not executing. This is because R understands the end of a line (i.e. a carriage return) as being the end of the command.

p + geom_point() + 
  geom_smooth() +
  scale_x_log10() # adjust the x axis by converting to log (base 10) values
# QUESTION: what have we done by converting the x axis to a log scale?

p + geom_point() + 
  geom_smooth() +
  scale_x_log10(labels = scales::dollar) # adjust the x axis labels by including dollar values instead of scientific notation
# Note the use of 'scales::dollar' in the above code - this calls on the dollar() function from the scales library, without having to load the entire library.

# We can manually adjust the aesthetic of the graph by including it as an argument of the geom() function i.e. it goes outside of aes().
# Here is some advice from Grolemund & Wickham (2017):
#	- color = "name of color" i.e. is a string
#	- size = number in mm
#	- shape = number between 0 and 24

p + geom_point(colour = "purple") + 
  geom_smooth() +
  scale_x_log10(labels = scales::dollar) # adjust the colour of the plotted points; note how it is included in the geom() funciton, NOT aes()
# TASK: change the colour and size of the trend line in the above plot (hint: size takes an integer as its value, not a string).

# Finally, let's add some labels and descriptions of the plot's various components:

p + geom_point(alpha = 0.3) + 
  geom_smooth() +
  scale_x_log10(labels = scales::dollar) +
  labs(x = "GDP Per Capita", y = "Life Expectancy in Years",
       title = "Economic Growth and Life Expectancy",
       subtitle = "Data points are country-years",
       caption = "Source: Gapminder.")
# QUESTION: what effect does the 'alpha = 0.3' option have on the plot?

# TASK: disaggregate the above plot by a third variable: 'continent' (hint: you need to edit the p object).

# ggplot expects your data to be tidy:
#	- every variable is a column
#	- every observation is a row

# A final note on ggplot(): we can perform the mapping of variables to aesthetic in the geom_() function. This is particularly
# useful when disaggregating by additional variables i.e. the points can be coloured by a third variable, while the trend line remains 
# unaffected.


# 4. Saving graphs #

# We often want to save plots individually for inclusion in papers, presentations, sharing with colleagues etc. ggplot() makes this easy:

p_out <- p + geom_point(alpha = 0.3) + 
  geom_smooth() +
  scale_x_log10(labels = scales::dollar) +
  labs(x = "GDP Per Capita", y = "Life Expectancy in Years",
       title = "Economic Growth and Life Expectancy",
       subtitle = "Data points are country-years",
       caption = "Source: Gapminder.") # store the full plot as a new object 'p_out'

ggsave("mylovelygraph.png", plot = p_out, height = 8, width = 12) # the first argument is the file path and name, the second is the name of the plot to save,
# the third and fourth control the size (in inches) of the image (though you can add a fifth argument ('units = ') to be specific.

ggsave("mylovelygraph.pdf", plot = p_out, height = 8, width = 12) # save as pdf


# 5. Data management #

# We often need to get our data in a format suitable for plotting: we might need to 
# collapse our data set to create aggregate values (e.g. yearly trends), or compute
# summary statistics from the raw data, or disaggregate our data into different pieces.

# In this section you will learn how to perform some crucial data management tasks using ggplot
# and R more generally. Using ggplot, we will focus on 'grouping', 'faceting' and 'transforming'
# our data for plotting.


# 5.1. Grouping
# Let's try and produce a graph of life expectency trajectory for each country over time:
library(gapminder) # load in gapminder data - see https://www.gapminder.org/
gapminder

p <- ggplot(data = gapminder,
            mapping = aes(x = year,
                          y = lifeExp))
p + geom_line() 
# QUESTION: what do you think has gone wrong here? Hint: take a look at the rows in the data.
# If you're looking for comfort in your graphing troubles, check out @accidental__aRt on Twitter...

# Let's see if we can correct the issue using the 'group' aesthetic:
p <- ggplot(data = gapminder,
            mapping = aes(x = year,
                          y = lifeExp))
p + geom_line(aes(group=country))
# That looks more like it: one line per country. It's still pretty rough-looking but we'll
# learn how to spruce it up later.
# QUESTION: how would you interpret the trend in life expectency over time?

# TASK: try grouping by another variable of your choice.

# The 'group' aesthetic is only necessary if the information ggplot needs is not built-in
# to the variables being plotted.


# 5.2 Faceting

# This is a very useful function for creating separate plots in the one visualisation.
# We do this by including a third variable in the ggplot command.

# For example, the plot of life expectency we produced above is cluttered and it would be useful to separate lines
# by continent to see if there are regional differences in life expectency.
x11()
p <- ggplot(data = gapminder,
            mapping = aes(x = year,
                          y = lifeExp))

p + geom_line(aes(group=country)) +
  facet_wrap(~ continent)
# TASK: describe the results of this graph.
# We use the '~' symbol to tell the facet_wrap command that we are providing it with a
# formula - in this case just a single variable but we could add more.

p + geom_line(aes(group=country)) +
  facet_wrap(~ continent, ncol = 5)
# QUESTION: what happens when we add the 'ncols' argument to the facet?

# Let's produce a cleaner graph:
p <- ggplot(data = gapminder, mapping = aes(x = year, y = lifeExp))

p + geom_line(color="gray70", aes(group = country)) +
  geom_smooth(size = 1.1, method = "loess", se = FALSE) +
  scale_y_log10(labels=scales::dollar) +
  facet_wrap(~ continent, ncol = 5) +
  labs(x = "Year",
       y = "Life Expectency",
       title = "Life Expectency on Five Continents")
# Still needs some work but not bad...

# facet_wrap() function is best used when you want multiple plots based on a single 
# categorical variable. If you a more complex graph, like one based on a contingency table,
# it is best to use the facet_grid() function.

# Load in the General Social Survey (GSS) 2016 data
library(socviz) # load in socviz suite of data sets and other resources
library(tidyverse) # load in tidyverse suite of packages

gss_sm
str(gss_sm)
glimpse(gss_sm) # we have a good deal more categorical variables we can work with
# See http://gss.norc.org/ for more information about the survey. It is roughly equivalent
# to the Understanding Society survey in the UK.

# Let's explore the association between respondent age and number of offspring:
p <- ggplot(data = gss_sm, mapping = aes(x = age, y = childs))

x11()
p + geom_point(alpha = 0.2) +
  geom_smooth() +
  facet_grid(sex ~ race)
# Note the syntax of facet_grid(): we are essentially saying "give us a plot of age and offspring,
# BY sex and race.
# QUESTION: what can you say about the relationship between age and offspring, and how it
# is mediated by a respondent's sex and race?

x11()
p + geom_point(alpha = 0.2) +
  geom_smooth() +
  facet_grid (~ race + sex)
# QUESTION: what is the difference between using (sex ~ race) and (~ sex + race)?


# TASK: explore the gss_sm data set and produce a multi-way graph of two numeric variables
# disaggregated by one or more categorical variables. Write a note summarising the results
# of this graph.

# An alternative to facet_() functions is the subset() function:
str(midwest)
class(midwest$state)
print(as.factor(midwest$state)) # 5 states in this data set

oh_wi <- c("OH", "WI") # specify a list of states we are interested in

p <- ggplot(data = subset(midwest, subset = state %in% oh_wi),
            mapping = aes(x = percollege, fill = state))

p + geom_histogram(alpha = 0.4, bins = 20)
# TASK: describe each component of the above ggplot.


# 5.3 Transforming
# Some geom_() functions perform some work on the data before presenting the results.
# Every geom_() function has an associated stat_() function e.g. the geom_smooth() function
# allows us to specify different methods for drawing a line that summaries the data points.
# The reverse is also true: every stat_() function has a geom_() function.
# Let's dig into this in more detail:

p <- ggplot(data = gss_sm,
            mapping = aes(x = bigregion))
p + geom_bar() 
# geom_bar() calculates and plots the counts/frequencies for the categories; what if we
# wanted proportions instead?

p + geom_bar(mapping = aes(y = ..prop..))
# Behind the scenes geom_bar() has called on its default stat_() function to produce
# two temporary variables: ..count.. and ..prop..
# They are bookended by full stops to distinguish them from variables in the data set.
# QUESTION: why do you think each bar sums to 1?

# Let's try fixing the summing issue:
p + geom_bar(mapping = aes(y = ..prop.., group = 1)) # that's better
# Adding the 'group = 1' argument tells geom_bar() to treat the data set as a single group.

# Let's get more familiar with stat_() functions:
table(gss_sm$religion) # frequency table of religion variable

p <- ggplot(data = gss_sm,
            mapping = aes(x = religion, fill = religion))
p + geom_bar() + guides(fill = FALSE) 
# guides() controls the display of the legend; in this instance we've suppressed the display of this feature.

# Now let's disaggregate religion by a second categorical variable:
p <- ggplot(data = gss_sm,
            mapping = aes(x = religion, fill = religion))

p + geom_bar(mapping = aes(y = ..prop.., group = 1)) +
  facet_wrap(~ bigregion) +
  guides(fill = FALSE) 
# TASK: try and add colour and labels to the above graph.

# Let's look at histograms and their stat_() functions:
p <- ggplot(data = midwest,
            mapping = aes(x = area))
p + geom_histogram(bins = 10)
# TASK: change the number of bins and describe the effect on the display of the information.

# Transforming data is vary useful step in producing effective visualisations.
# However, it is often easier to transform the data prior to calling the ggplot command;
# we'll learn to do this soon [PROBABLY EARLIER IN THE WORKSHOP]


# 6. Communicating Analytical Results #

# In this section we employ our graphing skills and techniques to the results of statistical models.
# We can do this is two ways:
# - employ modelling techniques as part of the plotting process i.e. directly within geoms;
# - use specialist packages to capture the results of statistical models and pass them to the plot.

# 6.1 Creating models within plots

# We saw earlier in the workshop an example of this approach: using geom_smooth() to 
# display a line of best of fit on a scatterplot.

library(gapminder)
library(ggplot2)
gapminder

p <- ggplot(data = gapminder,
            mapping = aes(x = log(gdpPercap), y = lifeExp))

p + geom_point(alpha=0.1) +
  geom_smooth(color = "steelblue", fill="steelblue", method = "lm")

p + geom_point(alpha=0.1) +
  geom_smooth(color = "tomato", method = "lm", size = 1.2, 
              formula = y ~ splines::bs(x, 3), se = FALSE)

# TASK: describe the difference between the lines fitted for each of these graphs. Which one
# do you think fits the data better?

# 6.1.1 Fit multiple models on a single graph
# This is easily achieved by stacking multiple geom_smooth() functions:
p + geom_point(alpha=0.1) +
  geom_smooth(color = "steelblue", fill="steelblue", method = "lm") +
  geom_smooth(color = "tomato", method = "lm", size = 1.2, 
              formula = y ~ splines::bs(x, 3), se = FALSE)

# There is an extra step we need to take in order to display the legend; it doesn't automatically
# appear as the geom_smooth() functions are not logically connected e.g. they are not categories
# of a single variable.

# First, load in some color palettes
model_colors <- RColorBrewer::brewer.pal(3, "Set1")
model_colors 

# Second, adjust the geom_smooth() color and fill specifications
p1 <- p + geom_point(alpha=0.1) +
  geom_smooth(aes(color = "OLS", fill = "OLS"), method = "lm") +
  geom_smooth(aes(color = "Poly", fill = "Poly"), method = "lm", size = 1.2, 
              formula = y ~ splines::bs(x, 3), se = FALSE)

# Third, map the color palette to each geom_smooth()
p2 <- p1 + scale_color_manual(name = "Models", values = model_colors) +
  scale_fill_manual(name = "Models", values = model_colors) +
  theme(legend.position = "top")

x11()
p2


# 6.2 Saving and graphing model results

# In this section we demonstrate how to store and plot the results of three types of statistical models:
# - multiple regression (OLS)
# - logistic regression (binomial)
# - count regression (Poisson)

# There are a plethora of other models that can be estimated in R which we cannot cover here.
# If you are interested in learning more, you can access the materials from our Predictive Analytics
# course (ADD LINK!); we also recommend Gelman and Hill (2018), Data Analysis Using Regression and Multilevel/Hierarchical Models.

# 6.2.1 How model results are stored
# The good news: model results, just like tables, graphs, data frames etc, are stored as objects in R;
# the bad news is they have a slightly more complicated structure...

# First, let's regress life expectancy on a couple of independent variables
out <- lm(formula = lifeExp ~ gdpPercap + pop + continent,
          data = gapminder)
out

summary(out) # look at values of this object
str(out) # look at the internal structure of this object
attributes(out) # look at the attributes of this object

# The attributes() function gives us a better sense of the model results; let's explore some:
out$coefficients # model coefficients (in an unhelpful format though)
out$model # data underpinning the model
out$effects # estimated effects for each observation
out$df.residual # degrees of freedom
?lm # see the 'Values' section for a description of each element of the out object

# 6.2.1 Some general tips for presenting the results of statistical models

# 1. Focus on the substantive interpretation of your findings
# 2. Communicate model uncertainty
# 3. Show the underlying data where possible

# 6.3. Graphing statistical models
# We will make use of David Robinson's 'broom' package to help us extract model results
# and graph them with ggplot2.

library(broom)
?broom
vignette("broom")
# 'broom' is a supremely useful package that operates at three levels:
# - component-level i.e. model coefficients and significance values
# - observation-level i.e. fitted values for each observation in the data
# - model-level i.e. model summary statistics (e.g. R-squared, F test)

# 6.3.1 Component-level statistics

library(socviz)
mod_comp <- tidy(out, conf.int = TRUE)
mod_comp %>% round_df()

p <- ggplot(mod_comp, mapping = aes(x = term,
                                    y = estimate))

p + geom_point() + coord_flip() # produces a Cleveland plot of the model point estimates'
# not bad but we can refine the graph further:

mod_comp <- subset(mod_comp, term %nin% "(Intercept)") # remove the intercept from the model table
mod_comp$term <- prefix_strip(mod_comp$term, "continent") # remove the 'continent' prefix from the values of 'term'

p <- ggplot(mod_comp, mapping = aes(x = reorder(term, estimate),
                                    y = estimate, ymin = conf.low, ymax = conf.high))

p + geom_pointrange() + coord_flip()

# TASK: describe what each component of the above plot is doing.
# TASK: add informative labels to the above graph, and change the size and color of the points.

# 6.3.2 Observation-level statistics

mod_obs <- augment(out)
View(mod_obs)
str(mod_obs)
# TASK: describe the variables/attributes of the mod_obs object e.g. what do you think 'fitted' represents?
# HINT: look at the help and vignette files for broom.

# Note how the augment() function only included variables from the original data set that were
# included in the model; we can override this default specification: 
mod_obs2 <- augment(out, data = gapminder)
View(mod_obs2)
str(mod_obs2)
# augment() returns the following additional variables (Healy, 2019):
# .fitted — The fitted values of the model.
# .se.fit — The standard errors of the fitted values.
# .resid — The residuals.
# .hat — The diagonal of the hat matrix.
# .sigma — An estimate of residual standard deviation when the corresponding observation is dropped from the model.
# .cooksd — Cook’s distance, a common regression diagnostic; and
# .std.resid — The standardized residuals.


# We can now graph the observation-level results of the model:
p <- ggplot(data = mod_obs,
            mapping = aes(x = .fitted, y = .std.resid))
p + geom_point()

# QUESTION: do you think a basic OLS is a good fit for the data: whay or why not?
# TASK: graph predicted values of the outcome against other variables in the mod_obs table.


# 6.3.3 Model-level statistics

glance(out) # produces a table of model summary statistics

# This doesn't look immediately useful - why would we want to graph the summary statistics for one model?
# However, the utility of glance() is evident when estimating and comparing multiple models.

# For example, let's estimate a model on observations relating to European countries in 1977:
eu77 <- gapminder %>% filter(continent == "Europe", year == 1977)

fit <- lm(lifeExp ~ log(gdpPercap), data = eu77)
summary(fit)

# TASK: produce and graph component and observation-level statistics for this model.

# Estimating models in this manner could lead to quite a bit of redundant code: for instance,
# we would have to create a subset of the data for every continent and year.
# Best if we use a new command, nest(), coupled with a piping command to create subsets automatically:
subsamp <- gapminder %>% 
  group_by(continent, year) %>%
  nest()

subsamp
# QUESTION: what do you think the values of the variable 'data' represent?
# TASK: explore the values and class of 'data' variable.

# We can also work backwards i.e. unnest the data set:
subsamp %>% 
  filter(continent == "Europe", year == 1977) %>%
  unnest()

# O.K. we're almost ready to estimate and graph multiple models
# First, create a function for estimating linear regressions:
fit_ols <- function(df) {
  lm(lifeExp ~ log(gdpPercap), data = df)
}

# The above is user-defined function called fit_ols; it takes one argument (df), which is used
# to identify the data underpinning the model.

# Second, we map the regression function to each row in the data column:
regout <- gapminder %>%
  group_by(continent, year) %>%
  nest() %>%
  mutate(model = map(data, fit_ols))

# Let's take a moment to unpick the above command:
# 1. We create an object to store the results of the piping command (regout)
# 2. We group the gapminder data by continent and year
# 3. We nest the rows underlying the group_by command 
#   e.g. Europe 1977 is made up of individual observations for multiple countries
# 4. We create a new list variable called model that stores the results of the regression commands

regout
regout$model
# QUESTION: how many regression commands were executed?

# Finally, we can extend our piping command to extract model summary statistics:
regsum <- gapminder %>%
  group_by(continent, year) %>%
  nest() %>%
  mutate(model = map(data, fit_ols)) %>%
  mutate(tidied = map(model, tidy)) %>% 
  unnest(tidied, .drop = TRUE) %>% 
  filter(term %nin% "(Intercept)" & continent %nin% "Oceania")

regsum
summary(regsum)
# TASK: describe the additional commands we have included in the above code i.e. from the second mutate command to the end.

# Phew! Lot of clever work has gone into getting some model summary statistics ready for the fun part: plotting :)

p <- ggplot(data = regsum, mapping = aes(x = year, y = estimate,
  ymin = estimate - 2*std.error,
  ymax = estimate + 2*std.error,
  group = continent, color = continent))

x11()
p + geom_pointrange(position = position_dodge(width = 1)) +
  scale_x_continuous(breaks = unique(gapminder$year)) + 
  theme(legend.position = "top") +
  labs(x = "Year", y = "Estimate", color = "Continent")

# QUESTION: how would you characterise the effect of GDP on life expectancy,
# over time and across continents?

# 6.3.4 Graphing logistic model results

library(margins)

gss_sm$polviews_m <- relevel(gss_sm$polviews, ref = "Moderate")

out_bo <- glm(obama ~ polviews_m + sex*race,
              family = "binomial", data = gss_sm)
summary(out_bo)

bo_m <- margins(out_bo)
summary(bo_m)
plot(bo_m)

##
  # Come back to this section with different data (maybe charity admin data?)
##

# As you can probably surmise, graphing the results of statistical models is tricky, in as much
# as the plotting elements are fairly standard but you must have a good grasp of the 
# purpose, mechanics and results of the model being estimated. 

# ggplot() is not your only option and we encourage to explore the use of base R plots
# and the coefplot library.

# There are additional issues we were unable to cover, such as graphing estimates from 
# complex surveys, and we refer you to chapter 6 of Healy (2019) and REFERENCE for further information.


# 7. Refining Graphs #

# In this section we build on the foundations we've laid earlier by adding layers of sophistication to our plots.
# We will learn new geom_() functions and how to combine them effectively; move away from ggplot's default options and
# start customising our graphs; and delve deeper into the scale(), guide() and theme() functions.

# 7.1 New geom() plots

# Representing multiple histograms in a single plot is made easy by using the geom_freqpoly() function:
library(gapminder)

p <- ggplot(data = gapminder,
            mapping = aes(x = lifeExp, colour = continent))

p + geom_freqpoly() # represents a histogram using lines instead of bars but is otherwise equivalent
# TASK: change the bandwidth of the histograms
# QUESTION: can you make meaninful comparisons between the distributions of the life expectancy for each continent? Why or why not?

p <- ggplot(data = gapminder,
            mapping = aes(x = lifeExp, y = ..density.., colour = continent))

p + geom_freqpoly() 
# TASK: describe what effect 'y = ..density..' has on the graph, especially in terms of making comparisons between distributions.


# 7.1.1 Dealing with two y-axes
# It's usually not good practice to plot lines representing different time series on two axes - 
# there is too much potential for misleading e.g. altering the y scales to display an
# apparent correlation.

# Instead, you can do the following:
# - display the line graphs side-by-side (i.e. use faceting)
# - create an index of each time series and plot both lines on the same y-axis


# We can zoom in on a plot using coord_cartesian():
contdata <- gapminder %>%
	group_by(continent, year) %>%
	summarise(mn_lifeExp = mean(lifeExp), mn_loggdp = mean(log2(gdp)))

p <- ggplot(data = contdata,
		mapping(aes(x = mn_loggdp, y = mn_lifeExp)

p + geom_point() # let's zoom in a particular area of the plot:
p + geom_point() + coord_cartesian(ylim = c(0, 50))
# This approach doesn't throw away any data, it simply focuses on a particular range of values for either the y or x axis.

# To restrict the range of values included in the plot, we use the xlim() and ylim() functions:
p + geom_point() + ylim(0, 50) # this removes observations with values outwith the range specified

# Of course, you can always filter your data to produce a subset of observations in a desired range.

# A tile plot
diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
    geom_tile(mapping = aes(fill = n))
# TASK: describe the plot produced by this command; what does it tell you about combinations of diamond colour and quality?


# 7.3.1 Using colour effectively via the scale() function

# We spoke early in the workshop about using colours intelligently. For example,
# the categories of an unordered categorical variable require distinct colours, while
# an ordered variable is better served by the use of gradation.
# The default options of ggplot() are usually appropriate but we may want to specify our
# own preferences via the scale() function and the RColorBrewer package.

# Let's explore different colour palettes using the organdata data set:
install.packages("RColorBrewer")
library(RColorBrewer)

x11()
display.brewer.all() # view the palettes available via RColorBrewer
# Notice we have three types of palettes:
# 1. Sequential
# 2. Qualitative
# 3. Divergent

p <- ggplot(data = organdata,
            mapping = aes(x = roads, y = donors, color = world))

p + geom_point(size = 2) + scale_color_brewer(palette = "Set2") +
  theme(legend.position = "top")

p + geom_point(size = 2) + scale_color_brewer(palette = "Pastel2") +
  theme(legend.position = "top")

p + geom_point(size = 2) + scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "top")

# TASK: use different palettes with the above graphs; which is best for converying differences
# between types of countries?

# It is possible to create your own colour palettes in R; see this article for more information:
# https://data.library.virginia.edu/setting-up-color-palettes-in-r/


# 7.3.2 Changing the appearance of a graph via the theme() function

# Combining ggplot() and the theme() function grants us considerable flexibility in how
# we construct the design elements of the plot. It also allows us to adopt standardised
# themes for our graphs e.g. Economist or Wall Street Journal graphics.

# First things first: let's get rid of the awful grey grid background of our graphs...
p <- ggplot(data = organdata,
            mapping = aes(x = roads, y = donors, color = world))

p + geom_point(size = 2) +
  theme_bw() +
  theme(legend.position = "top",
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())

p + geom_point(size = 2) +
  theme_minimal() +
  theme(legend.position = "top")

p + geom_point(size = 2) +
  theme_classic() +
  theme(legend.position = "top")

p + geom_point(size = 2) +
  theme_grey() +
  theme(legend.position = "top")

# Let's load in some new themes and apply them:
install.packages("ggthemes")
library(ggthemes)

p + geom_point(size = 2) +
  theme_economist() +
  theme(legend.position = "top")

# It is also possible to adjust the default options associated with a theme:
p + geom_point(size = 2) +
  theme_economist() +
  theme(legend.position = "top",
        legend.title = element_text(size = 30)) # adjust the size of the legend title

# "Flashy" themes like the Economist or WSJ are all well and good for one-off posters etc
# but you may want to focus on plainer plots for academic publications. The cowplot package
# provides a theme suitable for such publications.
install.packages("cowplot")
library(cowplot)
?cowplot # the help documentation is not very useful!!!

p + geom_point(size = 2) +
  theme_cowplot() +
  theme(legend.position = "top")






# 7.1 Summarising data
# It is usually more awkward and time consuming to rely on geom stat_() functions to summarise our variables
# during the graphing process. It is better to get into the habit of preparing our data in advance of plotting,
# using the very handy 'dplyr' package that comes with the 'tidyverse' distribution.

# For example, say we want to plot the results of a frequency table:
library(socviz) # load in socviz suite of data sets and other resources
library(tidyverse) # load in tidyverse suite of packages

gss_sm
str(gss_sm)
glimpse(gss_sm)

table(gss_sm$bigregion, gss_sm$religion) # crosstab of our two variables of interest

# To build our summary table, we need to perform a sequence of tasks on our data set
# using the pipe operator %>%.
# Think of this process as taking data as an input, transferring it into some functions,
# and being converted into some results (like a pipeline). In the words of Healy (2019):
# "A pipeline is typically a series of operations that do one or more of four things:
#   1. Group the data into the nested structure we want for our summary, such as “Religion by Region” or “Authors by Publications by Year”.
#   2. Filter or select pieces of the data by row, column, or both. This gets us the piece of the table we want to work on.
#   3. Mutate the data by creating new variables at the current level of grouping. This adds new columns to the table without aggregating it.
#   4. Summarize or aggregate the grouped data. This creates new variables at a higher level of grouping. For example we might calculate means with mean() or counts with n(). This results in a smaller, summary table, which we might do more things on if we want."

vignette("dplyr") # explore some use cases employing the 'dplyr' commands

# Let's create a crosstab of religion by region:
rel_by_reg <- gss_sm %>% 
  group_by(bigregion, religion) %>%
  summarize(N = n()) %>%
  mutate(freq = N / sum(N), pct = round((freq*100), 0))

View(rel_by_reg) # display the summary table we created

# There's a lot going on in the above command, so let's take it piece-by-piece:
#   1. we create a new object
#   2. state which data set we are wanting to use
#   3. take that data and group it by region and religion (similar to the table() command earlier)
#   4. summarise this table by creating a new column (N) which is a count of observations
#   5. create two new variables: proportion and percentage of observations in each category

# If you were apprehensive about learning or using R, the above command is a valid reason:
# it is more cumbersome than performing the same task in Stata or SPSS, and requires us
# to explicitly delineate the logic and steps of producing the summary table.

# There are user-written packages that can produce crosstabs more efficiently than the above
# method (e.g. 'crosstab' from the 'descr' package) but using the pipe operator in this
# way is a very common task in R and you just have to get used to it :)

# TASK: play around with the pipe command we specified (i.e rel_by_reg etc);
# try removing layers starting at the bottom - how does that effect the final table?

# Let's check if the percentages sum to 100 in our summary table:
sanity_check <- rel_by_reg %>%
  group_by(bigregion) %>%
  summarise(total = sum(pct))

sanity_check # some rounding errors but pretty much correct

# Let's plot the rel_by_reg object:
p1 <- ggplot(data = rel_by_reg, mapping = aes(x = bigregion, y = pct, fill = religion))

p1a + geom_col() +
  labs(x = "Region",y = "Percent", fill = "Religion") +
  theme(legend.position = "top") # not bad at all, but let's stack the bars side-by-side:

p1b + geom_col(position = "dodge2") +
  labs(x = "Region",y = "Percent", fill = "Religion") +
  theme(legend.position = "top")

# QUESTION: why did we use geom_col() instead of geom_bar()?

# I think we can do better; let's swap the x and y axes:
p2 <- ggplot(data = rel_by_reg, mapping = aes(x = pct, y = bigregion, fill = religion))

p2 + geom_col(position = "dodge2") +
  labs(x = "Region",y = "Percent", fill = "Religion") +
  theme(legend.position = "top") # ehh...

# geom_col has an internal logic that prevents simply swapping the axes in the mapping function.
# Thankfully there is another way:
p3 <- ggplot(data = rel_by_reg, mapping = aes(x = religion, y = pct, fill = religion))

p3 + geom_col(position = "dodge2") +
  labs(x = NULL ,y = "Percent", fill = "Religion") +
  guides(fill = FALSE) + # turns off the legend
  coord_flip() + # flips the co-ordinates of the plot i.e. swaps the axes (religion and pct)
  facet_grid(~ bigregion)

#QUESTION: in your opinion, which of the plots communicates the underlying patterns best (p1a, p1b, p2 or p3)?


# 6.2 Continuous data

# Let's apply the piping and plotting techniques to a different data set: organdata
# The data set contains more than a decade’s worth of information on the donation of organs for transplants in seventeen OECD countries.

organdata
glimpse(organdata)

# Let's explore a subset of the data set:
organdata %>% select(1:6) %>% sample_n(size = 20)
organdata %>% select(country, year, gdp) %>% sample_n(size = 5)

# TASK: describe the results of the above commands.

# TASK: produce a scatterplot of the mean number of organ donors by year and interpret the results.

# Let's explore country-level variation in the mean number of organ donors:
p <- ggplot(data = organdata, mapping = aes(x = country, y = donors))

p + geom_boxplot() +
  coord_flip() # pretty good but the lack of a suitable ordering makes it difficult to compare countries

p <- ggplot(data = organdata, 
            mapping = aes(x = reorder(country, donors, na.rm=TRUE), y = donors))

p + geom_boxplot() +
  coord_flip() +
  labs(x = NULL)

# QUESTION: what is the meaning of each component of the reorder() function?

# reorder() automatically calculates the mean of the second variable in the function.
# TASK: recreate the above boxplot but specify a different summary statistic (e.g. the median, 95th percentile, max etc)
# NB DMD: figure out how to perform the above TASK

p <- ggplot(data = organdata, 
            mapping = aes(x = reorder(country, donors, na.rm=TRUE), 
                          y = donors, fill = world))

p + geom_boxplot() +
  coord_flip() +
  labs(x = NULL) +
  theme(legend.position = "top")

# It is usually good practice to plot the distribution of categorical variables on the y axis.
# It's been interesting to look at the boxplot distribution of donors but perhaps we can
# make things simpler by focusing on a single value for this variable.

# First, create a summary table of the variables we want to plot:
country_analysis <- organdata %>% group_by(consent_law, country) %>%
  summarise(donors_mn = mean(donors, na.rm = TRUE),
            donors_sd = sd(donors, na.rm = TRUE),
            gdp_mn = mean(gdp, na.rm = TRUE))

country_analysis

# TASK: describe the logic and results of the above piping command.

# Second, plot the summary table
p <- ggplot(data = country_analysis, 
            mapping = aes(x = reorder(country, donors_mn),
                          y = donors_mn, color = consent_law))

p + geom_point(size = 4) +
  coord_flip() +
  labs(x = NULL, y = "Number of donors (mean)", color = "Consent law") +
  theme(legend.position = "top")

# QUESTION: are there meaningful country-level differences in the mean number of organ donors?
# what about by countries with different consent laws?

# Well done, you have combined data wrangling skills with clear, sophisticated plotting specifications
# to produce an effective visualisation: a Cleveland dotplot. However, there is always room for improvement...
# Let's rationalise the code that produced our summary table:
country_analysis <- organdata %>% group_by(consent_law, country) %>%
  summarise_if(is.numeric, funs(mean, sd, median), na.rm = TRUE)

country_analysis
str(country_analysis)
View(country_analysis)

# TASK: describe the logic and results of the new 'country_analysis' piping command.

# The next improvement we can make is to produce separate plots by the 'consent_law' variable:
p <- ggplot(data = country_analysis, 
            mapping = aes(x = reorder(country, donors_mean),
                          y = donors_mean)) # remember the names changed when we allowed R to create the variables

p + geom_point(size = 4) +
  coord_flip() +
  facet_wrap(~ consent_law)
labs(x = NULL, y = "Number of donors (mean)") +
  theme(legend.position = "top") # side-by-side is not very legible; this can be fixed by using the 'ncol' argument as part of facet_wrap()


p + geom_point(size = 4) +
  coord_flip() +
  facet_wrap(~ consent_law, ncol = 1)
labs(x = NULL, y = "Number of donors (mean)") +
  theme(legend.position = "top") # repetiion of country names across panels; we need to free the y axis to select its own values


p + geom_point(size = 4) +
  coord_flip() +
  facet_wrap(~ consent_law, ncol = 1, scales = "free_y")
labs(x = "Number of donors (mean)", y = NULL) +
  theme(legend.position = "top") 

# TASK: play around with 'ncol' and 'scales' options.

# As intelligent analysts, you may be critical of the above graph: "Instructor, what use is a
# point estimate without some knowledge of its variance or error?"
# Well my learned friends, ggplot() provides just the geom:
x11()
p + geom_pointrange(mapping = aes(ymin = donors_mean - donors_sd, ymax = donors_mean + donors_sd)) +
  coord_flip() +
  facet_wrap(~ consent_law, ncol = 1, scales = "free_y")
labs(x = NULL, y = "Number of donors (mean)") +
  theme(legend.position = "top") 

# TASK: adjust the axis labels to remove the defaulting labelling.








# 7. Working with Data #
# Getting data into the right shape for plotting. [ADD SOME OF THIS MATERIAL TO THE BEGINNING]


# 8. Refining Plots #

# 8.1 Plotting text

library(gapminder)
gapminder
glimpse(gapminder)

table(gapminder$continent)
levels(gapminder$continent)

# Let's examine life expectancy
lexp_analysis <- gapminder %>%
  filter(pop < 5000000) %>%
  group_by(country) %>%
  summarise(lexp_mean = mean(lifeExp, na.rm = TRUE),
            pop_mean = mean(pop, na.rm = TRUE))

# TASK: describe what the above piping command is doing.

p <- ggplot(data = lexp_analysis,
            mapping = aes(x = pop_mean, y = lexp_mean))

x11()
p + geom_point() + geom_text(mapping = aes(label = country)) # doesn't look great; let's make some tweaks

p + geom_point() + geom_text(mapping = aes(label = country), hjust = 0) # right-justify the text

# The default geom_text() function is a bit awkward to use; better to employ a user-written package called ggrepel()
install.packages("ggrepel")
library(ggrepel)

# U.S. presidential election results 1824-2016
elections_historic
str(elections_historic)

elections_historic %>% select(2:7, 10) # examine a subset of the variables and rows

# Let's produce a scatterplot of win share of the popular vote and electoral college vote
# First, define some objects to store the labels
p_title <- "Presidential Elections: Popular & Electoral College Margins"
p_subtitle <- "1824-2016"
p_caption <- "Data for 2016 are provisional."
x_label <- "Winner's share of Popular Vote"
y_label <- "Winner's share of Electoral College Votes"

# Second, define the data and variables
p <- ggplot(elections_historic, aes(x = popular_pct, y = ec_pct,
                                    label = winner_label))

# Third, produce the plot
x11()
p + geom_hline(yintercept = 0.5, size = 1.4, color = "gray80") +
  geom_vline(xintercept = 0.5, size = 1.4, color = "gray80") +
  geom_point() +
  geom_text_repel() +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = x_label, y = y_label, title = p_title, subtitle = p_subtitle,
       caption = p_caption)

# QUESTION: what can you conclude (if anything) about the relationship between popular and electoral college win shares?
# TASK: search for help on the geom_text_repel() function; play around the with above graph by altering the each element.

# 8.2. Labelling outliers

# Sometimes we just want to highlight particular data points or cases. We can use geom_text_repel()
# to do this by telling it to use a subset of the data:
x11()
p + geom_hline(yintercept = 0.5, size = 1.4, color = "gray80") +
  geom_vline(xintercept = 0.5, size = 1.4, color = "gray80") +
  geom_point() +
  geom_text_repel(data = subset(elections_historic, popular_pct < .5 & ec_pct > .7)) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = x_label, y = y_label, title = p_title, subtitle = p_subtitle,
       caption = p_caption)

# TASK: describe the effect of the addition of subset() to the plot.

# An alternative approach is to create an indicator (dummy) variable and subset by it.

# 8.3. Writing and drawing in the plot area

# Sometimes we are interested in adding our own annotations to the graph, perhaps to 
# pick out surprising data points or to add some commentary to the patterns observed.

x11()
p + geom_hline(yintercept = 0.5, size = 1.4, color = "gray80") +
  geom_vline(xintercept = 0.5, size = 1.4, color = "gray80") +
  geom_point() +
  annotate(geom = "text", x = .35, y = .45, label = "The unpopular quadrant", hjust = 0) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = x_label, y = y_label, title = p_title, subtitle = p_subtitle,
       caption = p_caption)

# TASK: describe the logic and results of the annotate() function.

# We can also draw shapes on the plot:
x11()
p + geom_hline(yintercept = 0.5, size = 1.4, color = "gray80") +
  geom_vline(xintercept = 0.5, size = 1.4, color = "gray80") +
  geom_point() +
  annotate(geom = "rect", xmin = .60, xmax = .65, ymin = .80, ymax = .99, fill = "blue", alpha = .3)
scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = x_label, y = y_label, title = p_title, subtitle = p_subtitle,
       caption = p_caption)






# Combining Individual Plots #


# Color Considerations #

# Select a different colour palette:
ggplot(data = auto) + 
  geom_point(mapping = aes(x =auto$price, y = auto$mpg, shape = auto$foreign, color = factor(auto$rep78))) +
  scale_color_brewer(palette = "Spectral")
# QUESTION: what did we need to add the factor prefix to auto$rep78; what happens if you remove the prefix?


# Labelling Plots #

ggplot(data = auto) + 
  geom_histogram(mapping = aes(auto$mpg, fill = auto$foreign), binwidth = 4) +
  labs(x = "Miles Per Gallon (MPG)", y = "No. of observations",
       title = "Fuel Efficiency", fill = "Region of Origin")

# TASK: create a scatter plot of two numeric variable and distinguish observations by a 
# third variable using the colour option. Add appropriate labels (hint: replace fill with colour
# in the above graph to change the text).


# Storing and Reusing Plots #

# Helpfully, R allows us to save the results of a plot as an object.
plt1 <- ggplot(data = auto) + geom_histogram(mapping = aes(auto$mpg, fill = auto$foreign), binwidth = 4)
plt1
# This approaches comes in useful when combining different plots in a single visualisation.


# Final Thoughts #

# Congratulations on progressing through the workshop. Our aim was to equip you with a proficiency in data visualisation using R as rapidly and painlessly as possible.
# While you have covered a great deal of material and skills, there is a bigger and badder world of R programming and graphing out there.
# Here are some suggestions for where you might go next in your skills development:
#	- Wanting to delve deeper into ggplot2? Consult ggplot2: Elegant Graphics for Data Analysis [https://www.amazon.com/dp/331924275X/ref=cm_sw_su_dp]	
#
#	- Looking for new plot types? Check out [https://www.ggplot2-exts.org/], which keeps track of ggplot2 extensions developed by R users.
#	(They even have a theme that makes your R plots look like Stata graphs, in case you're having withdrawal symptoms)

#	- We didn't cover an increasingly prominent area of data visualisation: graphing spatial data.
#		o Chapter 7 of Data Visualization: A Practical Introduction demonstrates how to use ggplot for drawing maps [http://socviz.co/maps.html#maps]
#		o The U.K. Consumer Data Research Centre (CDRC) provides introductory material on using R for working with spatial data [https://data.cdrc.ac.uk/dataset/an-introduction-to-spatial-data-analysis-and-visualisation-in-r]

#	- Data management is the most time consuming (and IMO, difficult) aspect of quantitative data analysis. There are some good (sometimes free) resources
#	that will help you develop your proficiency in this crucial task:
#		o R for Data Science [https://r4ds.had.co.nz]
#		o R Programming for Data Science [https://www.cs.upc.edu/~robert/teaching/estadistica/rprogramming.pdf]
#		o Data Wrangling with R [https://leanpub.com/datawranglingwithr]


################################################## FIN ##################################################